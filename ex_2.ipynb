{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Bigdata_Pyspark_Spark_Hadoop_Apache/blob/main/ex_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TL;DR\n",
        "Collaborators: Shoval Benjer 319037404, Adir Amar 209017755\n",
        "\n",
        "The Kafka-Spark pipeline successfully consumed sentiment data from the sentiments topic, processing words and their sentiment scores in real time. After receiving 10 messages, the consumer stopped as configured, and producer threads were gracefully terminated. This demonstrates the system's ability to handle streaming data efficiently within predefined limits."
      ],
      "metadata": {
        "id": "y6ymplfFAK_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup**\n",
        "\n",
        "**System Requirements:**\n",
        "\n",
        "    Operating System: Linux-based environment (recommended for compatibility) or Windows with WSL2.\n",
        "\n",
        "Software:\n",
        "    Python 3.8+, Java 8 (OpenJDK 8), and Apache Spark.\n",
        "\n",
        "Libraries:\n",
        "\n",
        "    pyspark, kafka-python, threading, and json.\n",
        "\n",
        "Environment Setup:\n",
        "\n",
        "**Google Colab is Recommended for running the notebook.**\n",
        "\n",
        "Local System: Ensure you have Apache Spark and Kafka installed with appropriate environment variables configured.\n",
        "\n",
        "\n",
        "Description for Each Step:\n",
        "\n",
        "      Install Java:\n",
        "      This command installs the OpenJDK 8 runtime environment, a necessary dependency for running Apache Spark and Kafka. The -qq flag minimizes output during the installation process.\n",
        "\n",
        "      Download Apache Spark:\n",
        "      Downloads Apache Spark version 3.5.0 with Hadoop 3 compatibility from the official Apache archives. Spark is a distributed computing framework essential for big data processing tasks.\n",
        "\n",
        "      Verify the Spark Download:\n",
        "      Lists the downloaded Spark tarball to confirm that the file has been successfully downloaded.\n",
        "\n",
        "      Extract the Spark Archive:\n",
        "      Unpacks the Spark tarball to make the Spark distribution files accessible for configuration and usage.\n",
        "\n",
        "      Move Spark to the Local Directory:\n",
        "      Moves the extracted Spark directory to /usr/local/spark, setting a standard location for Spark installation, simplifying environment variable configuration.\n",
        "\n",
        "      Download Apache Kafka:\n",
        "      Downloads Apache Kafka version 3.5.1 (Scala version 2.13), a distributed event-streaming platform commonly used for real-time data pipelines and streaming applications.\n",
        "\n",
        "      Verify the Kafka Download:\n",
        "      Lists the downloaded Kafka tarball to ensure successful file retrieval.\n",
        "\n",
        "      Extract the Kafka Archive:\n",
        "      Unpacks the Kafka tarball to access its binaries and configuration files.\n",
        "\n",
        "      Move Kafka to the Local Directory:\n",
        "      Moves the extracted Kafka directory to /usr/local/kafka for organized setup and easier configuration.\n",
        "\n",
        "      Set Environment Variables:\n",
        "      Configures environment variables for Java, Spark, and Kafka to ensure their executables can be accessed system-wide. This includes updating the PATH variable for seamless command-line operations.\n",
        "\n",
        "      Install Python Libraries:\n",
        "      Installs pyspark for interacting with Spark using Python and kafka-python for Kafka integration within Python applications.\n",
        "\n",
        "      Start Zookeeper:\n",
        "      Launches Zookeeper, a centralized service used by Kafka for managing distributed systems. It provides configuration synchronization and group services for Kafka brokers.\n",
        "\n",
        "      Start Kafka Broker:\n",
        "      Starts the Kafka broker service, which handles message queuing, storage, and distribution to clients in a publish-subscribe model."
      ],
      "metadata": {
        "id": "Ej_040DT-oLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!ls -l spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!mv spark-3.5.0-bin-hadoop3 /usr/local/spark\n",
        "!wget -q https://archive.apache.org/dist/kafka/3.5.1/kafka_2.13-3.5.1.tgz\n",
        "!ls -l kafka_2.13-3.5.1.tgz\n",
        "!tar xf kafka_2.13-3.5.1.tgz\n",
        "!mv kafka_2.13-3.5.1 /usr/local/kafka\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark\"\n",
        "os.environ[\"PATH\"] += \":/usr/local/spark/bin\"\n",
        "os.environ[\"PATH\"] += \":/usr/local/kafka/bin\"\n",
        "!pip install pyspark kafka-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5x2Y8YUGd4d",
        "outputId": "2d6428de-bdb4-4b72-bce1-e83e3a4a362e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "im feeling fantastic!-rw-r--r-- 1 root root 400395283 Sep  9  2023 spark-3.5.0-bin-hadoop3.tgz\n",
            "mv: cannot move 'spark-3.5.0-bin-hadoop3' to '/usr/local/spark/spark-3.5.0-bin-hadoop3': Directory not empty\n",
            "-rw-r--r-- 1 root root 106748875 Jul 21  2023 kafka_2.13-3.5.1.tgz\n",
            "mv: cannot move 'kafka_2.13-3.5.1' to '/usr/local/kafka/kafka_2.13-3.5.1': Directory not empty\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.4)\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup /usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties &\n",
        "!nohup /usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDIMy5y0EJDW",
        "outputId": "eb5ab32d-2ac0-44b9-f37c-3a658378e7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Kafka Topics for Data Streams\n",
        "\n",
        "    This name accurately reflects the purpose of the snippet, which is to create Kafka topics (sentiments and text) for managing separate streams of data within the Kafka ecosystem."
      ],
      "metadata": {
        "id": "aA2JRESP_IPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kafka-topics.sh --create --topic sentiments --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1\n",
        "!kafka-topics.sh --create --topic text --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1"
      ],
      "metadata": {
        "id": "_g9pMW9zGGSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Producer for Sentiments (producer_sentiments):**\n",
        "\n",
        "    This function reads sentiment data from a file (e.g., AFINN-111.txt) and continuously sends key-value pairs representing words and their sentiment scores to a specified Kafka topic. It simulates real-time streaming by batching the data and rotating through the dataset. The Kafka producer uses JSON serialization to encode messages before sending them to the topic.\n",
        "\n",
        "**Producer for Text (producer_text):**\n",
        "\n",
        "    This function allows users to input text sentences via the console and sends them to a specified Kafka topic in real time. It uses a Kafka producer to serialize the text and send it as a message. This enables dynamic user interaction and real-time data streaming for text analysis.\n",
        "\n",
        "**Spark Kafka Consumer (spark_kafka_consumer):**\n",
        "\n",
        "    This function consumes messages from two Kafka topics: one for sentiment data and another for user text input. Using Apache Spark, it processes these messages to calculate the Total Sentiment Level (TSL) for user input based on the sentiment data. The consumer maintains a dictionary of word sentiments and updates it dynamically from the sentiment topic. It evaluates each input sentence for known sentiment words, computes the TSL, and outputs the results. The function also includes safeguards for JSON decoding and message processing errors, with a configurable limit for the number of messages to process before stopping."
      ],
      "metadata": {
        "id": "2M5wDhNx_Vnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kafka import KafkaProducer, KafkaConsumer\n",
        "from pyspark.sql import SparkSession\n",
        "import threading\n",
        "import time\n",
        "import json\n",
        "\n",
        "def producer_sentiments(file_path, topic, bootstrap_servers='localhost:9092'):\n",
        "    \"\"\"\n",
        "    Reads sentiment data from a file and sends it to the specified Kafka topic.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the sentiment file (e.g., 'AFINN-111.txt').\n",
        "        topic (str): Kafka topic to send the data to.\n",
        "        bootstrap_servers (str): Kafka server address. Default is 'localhost:9092'.\n",
        "    \"\"\"\n",
        "    producer = KafkaProducer(\n",
        "        bootstrap_servers=bootstrap_servers,\n",
        "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
        "    )\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    sentiment_data = [(line.split('\\t')[0], int(line.split('\\t')[1])) for line in lines]\n",
        "    while True:\n",
        "        batch = sentiment_data[:100]\n",
        "        for word, sentiment in batch:\n",
        "            producer.send(topic, {'word': word, 'sentiment': sentiment})\n",
        "        sentiment_data = sentiment_data[100:] + batch  # Rotate data\n",
        "        time.sleep(2)\n",
        "\n",
        "def producer_text(topic, bootstrap_servers='localhost:9092'):\n",
        "    \"\"\"\n",
        "    Reads user input from the console and sends it to the specified Kafka topic.\n",
        "\n",
        "    Args:\n",
        "        topic (str): Kafka topic to send the data to.\n",
        "        bootstrap_servers (str): Kafka server address. Default is 'localhost:9092'.\n",
        "    \"\"\"\n",
        "    producer = KafkaProducer(\n",
        "        bootstrap_servers=bootstrap_servers,\n",
        "        value_serializer=lambda v: v.encode('utf-8')\n",
        "    )\n",
        "    while True:\n",
        "        user_input = input(\"Enter a sentence to analyze: \")\n",
        "        producer.send(topic, user_input)\n",
        "        print(f\"Sent: {user_input}\")\n",
        "\n",
        "def spark_kafka_consumer(bootstrap_servers='localhost:9092', sentiment_topic='sentiments', text_topic='text', stop_after=10):\n",
        "    \"\"\"\n",
        "    Consumes messages from Kafka topics and calculates the Total Sentiment Level (TSL) using Spark.\n",
        "\n",
        "    Args:\n",
        "        bootstrap_servers (str): Kafka server address. Default is 'localhost:9092'.\n",
        "        sentiment_topic (str): Kafka topic for sentiment data.\n",
        "        text_topic (str): Kafka topic for text data.\n",
        "        stop_after (int): Number of messages to process before stopping. Default is 10.\n",
        "    \"\"\"\n",
        "    from kafka import KafkaConsumer\n",
        "    from pyspark.sql import SparkSession\n",
        "\n",
        "    # Start Spark session\n",
        "    spark = SparkSession.builder.appName(\"KafkaSparkConsumer\").getOrCreate()\n",
        "    sc = spark.sparkContext\n",
        "\n",
        "    # Create Kafka consumer\n",
        "    consumer = KafkaConsumer(\n",
        "        sentiment_topic,\n",
        "        text_topic,\n",
        "        bootstrap_servers=bootstrap_servers,\n",
        "        value_deserializer=lambda v: v.decode('utf-8'),  # Deserialize as string\n",
        "    )\n",
        "\n",
        "    # Dictionary to store sentiments\n",
        "    sentiment_dict = {}\n",
        "    message_count = 0\n",
        "\n",
        "    for message in consumer:\n",
        "        try:\n",
        "            topic = message.topic\n",
        "            value = message.value\n",
        "            print(f\"Received message from topic '{topic}': {value}\")\n",
        "\n",
        "            if topic == sentiment_topic:\n",
        "                # Parse JSON for sentiment data\n",
        "                sentiment_data = json.loads(value)\n",
        "                sentiment_dict[sentiment_data['word']] = sentiment_data['sentiment']\n",
        "            elif topic == text_topic:\n",
        "                # Process text data\n",
        "                words = value.split()\n",
        "                known_sentiments = [sentiment_dict[word] for word in words if word in sentiment_dict]\n",
        "                if known_sentiments:\n",
        "                    tsl = sum(known_sentiments) / len(known_sentiments)\n",
        "                    print(f\"TSL for '{value}': {tsl}\")\n",
        "                else:\n",
        "                    print(f\"TSL for '{value}': No known words in sentiment dictionary\")\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Error decoding JSON, skipping message:\", message.value)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing message: {e}\")\n",
        "\n",
        "        message_count += 1\n",
        "        if message_count >= stop_after:\n",
        "            print(\"Processed maximum messages. Stopping consumer...\")\n",
        "            break"
      ],
      "metadata": {
        "id": "F6viv9wHB968"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Function Execution (if __name__ == \"__main__\":):**\n",
        "\n",
        "    Acts as the entry point for the application, coordinating the execution of Kafka producers and the Spark Kafka consumer. It orchestrates the threads for producers and ensures proper shutdown after processing is complete.\n",
        "\n",
        "**Starting Kafka Producer for Sentiments (producer_sentiments_thread):**\n",
        "\n",
        "    Initializes a separate thread to execute the producer_sentiments function, which streams sentiment data from a file (AFINN-111.txt) to the Kafka sentiments topic in real time. The thread runs as a daemon, ensuring it stops when the main program ends.\n",
        "\n",
        "**Starting Kafka Producer for Text (producer_text_thread):**\n",
        "\n",
        "    Initializes another daemon thread to execute the producer_text function, which allows users to input text and streams the sentences to the Kafka text topic in real time. This thread operates independently, enabling simultaneous interaction with the consumer.\n",
        "\n",
        "**Running the Spark Kafka Consumer (spark_kafka_consumer):**\n",
        "\n",
        "    Starts the consumer process, which subscribes to Kafka topics (sentiments and text), processes messages, and calculates the Total Sentiment Level (TSL) for user input. The consumer stops after processing a configurable number of messages (stop_after=10), demonstrating batch processing within a controlled scope.\n",
        "\n",
        "**Stopping Producer Threads:**\n",
        "\n",
        "    After the consumer finishes processing, the main program ensures clean termination of the producer threads by joining them with a timeout. This step gracefully ends the producer processes and prevents lingering threads from running indefinitely.\n",
        "\n"
      ],
      "metadata": {
        "id": "Tstk7ix8_zzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "# Main function to start producers and consumer\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the sentiment file\n",
        "    sentiment_file = \"AFINN-111.txt\"\n",
        "\n",
        "    # Start the Kafka producer for 'sentiments' topic\n",
        "    producer_sentiments_thread = threading.Thread(\n",
        "        target=producer_sentiments, args=(sentiment_file, \"sentiments\")\n",
        "    )\n",
        "    producer_sentiments_thread.daemon = True\n",
        "    producer_sentiments_thread.start()\n",
        "\n",
        "    # Start the Kafka producer for 'text' topic\n",
        "    producer_text_thread = threading.Thread(\n",
        "        target=producer_text, args=(\"text\",)\n",
        "    )\n",
        "    producer_text_thread.daemon = True\n",
        "    producer_text_thread.start()\n",
        "\n",
        "    # Run the Kafka consumer for a limited number of messages\n",
        "    spark_kafka_consumer(stop_after=10)\n",
        "\n",
        "    # Stop producer threads after consumer finishes\n",
        "    print(\"Stopping producer threads...\")\n",
        "    producer_sentiments_thread.join(timeout=5)\n",
        "    producer_text_thread.join(timeout=5)\n",
        "    print(\"Producers stopped.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5aUb0wvHhMs",
        "outputId": "eb7d5b74-a403-42a2-dd6c-43fe8dda7d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:kafka.coordinator.consumer:group_id is None: disabling auto-commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received message from topic 'sentiments': {\"word\": \"boosts\", \"sentiment\": 1}\n",
            "Received message from topic 'sentiments': {\"word\": \"bore\", \"sentiment\": -2}\n",
            "Received message from topic 'sentiments': {\"word\": \"bored\", \"sentiment\": -2}\n",
            "Received message from topic 'sentiments': {\"word\": \"boring\", \"sentiment\": -3}\n",
            "Received message from topic 'sentiments': {\"word\": \"bother\", \"sentiment\": -2}\n",
            "Received message from topic 'sentiments': {\"word\": \"bothered\", \"sentiment\": -2}\n",
            "Received message from topic 'sentiments': {\"word\": \"bothers\", \"sentiment\": -2}\n",
            "Received message from topic 'sentiments': {\"word\": \"bothersome\", \"sentiment\": -2}\n",
            "Received message from topic 'sentiments': {\"word\": \"boycott\", \"sentiment\": -2}\n",
            "Received message from topic 'sentiments': {\"word\": \"boycotted\", \"sentiment\": -2}\n",
            "Processed maximum messages. Stopping consumer...\n",
            "Stopping producer threads...\n",
            "Producers stopped.\n"
          ]
        }
      ]
    }
  ]
}