{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Bigdata_Pyspark_Spark_Hadoop_Apache/blob/main/319037404_209017755.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Apache Spark\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# Verify the download\n",
        "!ls -l spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# Extract the downloaded file\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# Move Spark to /usr/local/spark\n",
        "!mv spark-3.5.0-bin-hadoop3 /usr/local/spark\n",
        "\n",
        "# Download Apache Kafka\n",
        "!wget -q https://archive.apache.org/dist/kafka/3.5.1/kafka_2.13-3.5.1.tgz\n",
        "\n",
        "# Verify the download\n",
        "!ls -l kafka_2.13-3.5.1.tgz\n",
        "\n",
        "# Extract the downloaded file\n",
        "!tar xf kafka_2.13-3.5.1.tgz\n",
        "\n",
        "# Move Kafka to /usr/local/kafka\n",
        "!mv kafka_2.13-3.5.1 /usr/local/kafka\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark\"\n",
        "os.environ[\"PATH\"] += \":/usr/local/spark/bin\"\n",
        "\n",
        "# Set Kafka environment variable\n",
        "os.environ[\"PATH\"] += \":/usr/local/kafka/bin\"\n",
        "\n",
        "# Install pyspark\n",
        "!pip install pyspark kafka-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5x2Y8YUGd4d",
        "outputId": "2d6428de-bdb4-4b72-bce1-e83e3a4a362e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "im feeling fantastic!-rw-r--r-- 1 root root 400395283 Sep  9  2023 spark-3.5.0-bin-hadoop3.tgz\n",
            "mv: cannot move 'spark-3.5.0-bin-hadoop3' to '/usr/local/spark/spark-3.5.0-bin-hadoop3': Directory not empty\n",
            "-rw-r--r-- 1 root root 106748875 Jul 21  2023 kafka_2.13-3.5.1.tgz\n",
            "mv: cannot move 'kafka_2.13-3.5.1' to '/usr/local/kafka/kafka_2.13-3.5.1': Directory not empty\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.4)\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Zookeeper\n",
        "!nohup /usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties &\n",
        "\n",
        "# Start Kafka Broker\n",
        "!nohup /usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDIMy5y0EJDW",
        "outputId": "eb5ab32d-2ac0-44b9-f37c-3a658378e7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 'sentiments' topic\n",
        "!kafka-topics.sh --create --topic sentiments --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1\n",
        "\n",
        "# Create 'text' topic\n",
        "!kafka-topics.sh --create --topic text --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g9pMW9zGGSx",
        "outputId": "c99d0e7d-f819-44dd-cffa-08b6cb6ba897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error while executing topic command : Topic 'sentiments' already exists.\n",
            "[2024-12-21 17:02:10,940] ERROR org.apache.kafka.common.errors.TopicExistsException: Topic 'sentiments' already exists.\n",
            " (kafka.admin.TopicCommand$)\n",
            "Error while executing topic command : Topic 'text' already exists.\n",
            "[2024-12-21 17:02:13,587] ERROR org.apache.kafka.common.errors.TopicExistsException: Topic 'text' already exists.\n",
            " (kafka.admin.TopicCommand$)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kafka import KafkaProducer, KafkaConsumer\n",
        "from pyspark.sql import SparkSession\n",
        "import threading\n",
        "import time\n",
        "import json\n",
        "\n",
        "def producer_sentiments(file_path, topic, bootstrap_servers='localhost:9092'):\n",
        "    \"\"\"\n",
        "    Reads sentiment data from a file and sends it to the specified Kafka topic.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the sentiment file (e.g., 'AFINN-111.txt').\n",
        "        topic (str): Kafka topic to send the data to.\n",
        "        bootstrap_servers (str): Kafka server address. Default is 'localhost:9092'.\n",
        "    \"\"\"\n",
        "    producer = KafkaProducer(\n",
        "        bootstrap_servers=bootstrap_servers,\n",
        "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
        "    )\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    sentiment_data = [(line.split('\\t')[0], int(line.split('\\t')[1])) for line in lines]\n",
        "    while True:\n",
        "        batch = sentiment_data[:100]\n",
        "        for word, sentiment in batch:\n",
        "            producer.send(topic, {'word': word, 'sentiment': sentiment})\n",
        "        sentiment_data = sentiment_data[100:] + batch  # Rotate data\n",
        "        time.sleep(2)\n",
        "\n",
        "def producer_text(topic, bootstrap_servers='localhost:9092'):\n",
        "    \"\"\"\n",
        "    Reads user input from the console and sends it to the specified Kafka topic.\n",
        "\n",
        "    Args:\n",
        "        topic (str): Kafka topic to send the data to.\n",
        "        bootstrap_servers (str): Kafka server address. Default is 'localhost:9092'.\n",
        "    \"\"\"\n",
        "    producer = KafkaProducer(\n",
        "        bootstrap_servers=bootstrap_servers,\n",
        "        value_serializer=lambda v: v.encode('utf-8')\n",
        "    )\n",
        "    while True:\n",
        "        user_input = input(\"Enter a sentence to analyze: \")\n",
        "        producer.send(topic, user_input)\n",
        "        print(f\"Sent: {user_input}\")\n",
        "\n",
        "def spark_kafka_consumer(bootstrap_servers='localhost:9092', sentiment_topic='sentiments', text_topic='text', stop_after=10):\n",
        "    \"\"\"\n",
        "    Consumes messages from Kafka topics and calculates the Total Sentiment Level (TSL) using Spark.\n",
        "\n",
        "    Args:\n",
        "        bootstrap_servers (str): Kafka server address. Default is 'localhost:9092'.\n",
        "        sentiment_topic (str): Kafka topic for sentiment data.\n",
        "        text_topic (str): Kafka topic for text data.\n",
        "        stop_after (int): Number of messages to process before stopping. Default is 10.\n",
        "    \"\"\"\n",
        "    from kafka import KafkaConsumer\n",
        "    from pyspark.sql import SparkSession\n",
        "\n",
        "    # Start Spark session\n",
        "    spark = SparkSession.builder.appName(\"KafkaSparkConsumer\").getOrCreate()\n",
        "    sc = spark.sparkContext\n",
        "\n",
        "    # Create Kafka consumer\n",
        "    consumer = KafkaConsumer(\n",
        "        sentiment_topic,\n",
        "        text_topic,\n",
        "        bootstrap_servers=bootstrap_servers,\n",
        "        value_deserializer=lambda v: v.decode('utf-8'),  # Deserialize as string\n",
        "    )\n",
        "\n",
        "    # Dictionary to store sentiments\n",
        "    sentiment_dict = {}\n",
        "    message_count = 0\n",
        "\n",
        "    for message in consumer:\n",
        "        try:\n",
        "            topic = message.topic\n",
        "            value = message.value\n",
        "            print(f\"Received message from topic '{topic}': {value}\")\n",
        "\n",
        "            if topic == sentiment_topic:\n",
        "                # Parse JSON for sentiment data\n",
        "                sentiment_data = json.loads(value)\n",
        "                sentiment_dict[sentiment_data['word']] = sentiment_data['sentiment']\n",
        "            elif topic == text_topic:\n",
        "                # Process text data\n",
        "                words = value.split()\n",
        "                known_sentiments = [sentiment_dict[word] for word in words if word in sentiment_dict]\n",
        "                if known_sentiments:\n",
        "                    tsl = sum(known_sentiments) / len(known_sentiments)\n",
        "                    print(f\"TSL for '{value}': {tsl}\")\n",
        "                else:\n",
        "                    print(f\"TSL for '{value}': No known words in sentiment dictionary\")\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Error decoding JSON, skipping message:\", message.value)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing message: {e}\")\n",
        "\n",
        "        message_count += 1\n",
        "        if message_count >= stop_after:\n",
        "            print(\"Processed maximum messages. Stopping consumer...\")\n",
        "            break"
      ],
      "metadata": {
        "id": "F6viv9wHB968"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "# Main function to start producers and consumer\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the sentiment file\n",
        "    sentiment_file = \"AFINN-111.txt\"\n",
        "\n",
        "    # Start the Kafka producer for 'sentiments' topic\n",
        "    producer_sentiments_thread = threading.Thread(\n",
        "        target=producer_sentiments, args=(sentiment_file, \"sentiments\")\n",
        "    )\n",
        "    producer_sentiments_thread.daemon = True\n",
        "    producer_sentiments_thread.start()\n",
        "\n",
        "    # Start the Kafka producer for 'text' topic\n",
        "    producer_text_thread = threading.Thread(\n",
        "        target=producer_text, args=(\"text\",)\n",
        "    )\n",
        "    producer_text_thread.daemon = True\n",
        "    producer_text_thread.start()\n",
        "\n",
        "    # Run the Kafka consumer for a limited number of messages\n",
        "    spark_kafka_consumer(stop_after=10)\n",
        "\n",
        "    # Stop producer threads after consumer finishes\n",
        "    print(\"Stopping producer threads...\")\n",
        "    producer_sentiments_thread.join(timeout=5)\n",
        "    producer_text_thread.join(timeout=5)\n",
        "    print(\"Producers stopped.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5aUb0wvHhMs",
        "outputId": "eb7d5b74-a403-42a2-dd6c-43fe8dda7d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:kafka.coordinator.consumer:group_id is None: disabling auto-commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received message from topic 'sentiments': {\"word\": \"boosts\", \"sentiment\": 1}\n",
            "Received message from topic 'sentiments': {\"word\": \"bore\", \"sentiment\": -2}\n",
            "Received message from topic 'sentiments': {\"word\": \"bored\", \"sentiment\": -2}\n",
            "Received message from topic 'sentiments': {\"word\": \"boring\", \"sentiment\": -3}\n",
            "Received message from topic 'sentiments': {\"word\": \"bother\", \"sentiment\": -2}\n",
            "Received message from topic 'sentiments': {\"word\": \"bothered\", \"sentiment\": -2}\n",
            "Received message from topic 'sentiments': {\"word\": \"bothers\", \"sentiment\": -2}\n",
            "Received message from topic 'sentiments': {\"word\": \"bothersome\", \"sentiment\": -2}\n",
            "Received message from topic 'sentiments': {\"word\": \"boycott\", \"sentiment\": -2}\n",
            "Received message from topic 'sentiments': {\"word\": \"boycotted\", \"sentiment\": -2}\n",
            "Processed maximum messages. Stopping consumer...\n",
            "Stopping producer threads...\n",
            "Producers stopped.\n"
          ]
        }
      ]
    }
  ]
}
