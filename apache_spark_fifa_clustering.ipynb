{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOQO84T1lQ+HIa1TBnnO3w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Bigdata_Pyspark_Spark_Hadoop_Apache/blob/main/apache_spark_fifa_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TL;DR**\n",
        "\n",
        "**Collaborators: Shoval Benjer 319037404, Adir Amar 209017755**\n",
        "\n",
        "This project demonstrates clustering analysis on FIFA player data using Apache Spark. It includes steps to preprocess data, apply feature transformations, find optimal clustering parameters, and visualize clusters with PCA.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QrEWcYg0suvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup**\n",
        "\n",
        "**System Requirements:**\n",
        "\n",
        "    Operating System: Linux-based environment (recommended for compatibility) or Windows with WSL2.\n",
        "\n",
        "Software:\n",
        "    Python 3.8+, Java 8 (OpenJDK 8), and Apache Spark.\n",
        "\n",
        "Libraries:\n",
        "\n",
        "    pyspark, kafka-python, threading, and json.\n",
        "\n",
        "Environment Setup:\n",
        "\n",
        "**Google Colab is Recommended for running the notebook.**\n",
        "\n",
        "Local System: Ensure you have Apache Spark and Kafka installed with appropriate environment variables configured.\n",
        "\n",
        "\n",
        "Description for Each Step:\n",
        "\n",
        "      Install Java:\n",
        "      This command installs the OpenJDK 8 runtime environment, a necessary dependency for running Apache Spark and Kafka. The -qq flag minimizes output during the installation process.\n",
        "\n",
        "      Download Apache Spark:\n",
        "      Downloads Apache Spark version 3.5.0 with Hadoop 3 compatibility from the official Apache archives. Spark is a distributed computing framework essential for big data processing tasks.\n",
        "\n",
        "      Verify the Spark Download:\n",
        "      Lists the downloaded Spark tarball to confirm that the file has been successfully downloaded.\n",
        "\n",
        "      Extract the Spark Archive:\n",
        "      Unpacks the Spark tarball to make the Spark distribution files accessible for configuration and usage.\n",
        "\n",
        "      Move Spark to the Local Directory:\n",
        "      Moves the extracted Spark directory to /usr/local/spark, setting a standard location for Spark installation, simplifying environment variable configuration.\n",
        "\n",
        "      Download Apache Kafka:\n",
        "      Downloads Apache Kafka version 3.5.1 (Scala version 2.13), a distributed event-streaming platform commonly used for real-time data pipelines and streaming applications.\n",
        "\n",
        "      Verify the Kafka Download:\n",
        "      Lists the downloaded Kafka tarball to ensure successful file retrieval.\n",
        "\n",
        "      Extract the Kafka Archive:\n",
        "      Unpacks the Kafka tarball to access its binaries and configuration files.\n",
        "\n",
        "      Move Kafka to the Local Directory:\n",
        "      Moves the extracted Kafka directory to /usr/local/kafka for organized setup and easier configuration.\n",
        "\n",
        "      Set Environment Variables:\n",
        "      Configures environment variables for Java, Spark, and Kafka to ensure their executables can be accessed system-wide. This includes updating the PATH variable for seamless command-line operations.\n",
        "\n",
        "      Install Python Libraries:\n",
        "      Installs pyspark for interacting with Spark using Python and kafka-python for Kafka integration within Python applications.\n",
        "\n",
        "      Start Zookeeper:\n",
        "      Launches Zookeeper, a centralized service used by Kafka for managing distributed systems. It provides configuration synchronization and group services for Kafka brokers.\n",
        "\n",
        "      Start Kafka Broker:\n",
        "      Starts the Kafka broker service, which handles message queuing, storage, and distribution to clients in a publish-subscribe model."
      ],
      "metadata": {
        "id": "xgSOce1_snEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup, and Load"
      ],
      "metadata": {
        "id": "89GHgG2n2doi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "2) IMPORTS & SPARK SESSION\n",
        "All required libraries, including Spark and matplotlib.\n",
        "\"\"\"\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import desc, col, regexp_replace\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "import matplotlib.pyplot as plt\n",
        "spark = SparkSession.builder.appName(\"FIFA_ClusteringClean\").getOrCreate()"
      ],
      "metadata": {
        "id": "MSAW59m51gr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads the FIFA data from a CSV file into a Spark DataFrame.\n",
        "    Parameters:\n",
        "        file_path (str): The path to the CSV file.\n",
        "    Returns:\n",
        "        DataFrame: Spark DataFrame containing the FIFA data.\n",
        "    \"\"\"\n",
        "    df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
        "    return df"
      ],
      "metadata": {
        "id": "6Cl494mX0-Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_salary_column(df):\n",
        "    \"\"\"\n",
        "    Cleans 'Wage' (e.g., '€565K') to create a numeric 'Salary' column.\n",
        "     - Removes '€'\n",
        "     - Removes 'K'\n",
        "     - Casts to double\n",
        "    Parameters:\n",
        "        df (DataFrame): The source DataFrame with 'Wage' as string.\n",
        "    Returns:\n",
        "        DataFrame: A new DataFrame with an extra numeric 'Salary' column.\n",
        "    \"\"\"\n",
        "    temp = df.withColumn(\"Salary\", regexp_replace(col(\"Wage\"), \"€\", \"\"))\n",
        "    temp = temp.withColumn(\"Salary\", regexp_replace(col(\"Salary\"), \"K\", \"\"))\n",
        "    temp = temp.withColumn(\"Salary\", col(\"Salary\").cast(\"double\"))\n",
        "    return temp\n"
      ],
      "metadata": {
        "id": "qrK5N01f1df5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_value_column(df):\n",
        "    \"\"\"\n",
        "    Cleans 'Value' (e.g., '€110.5M', '€405K', etc.) to make it numeric in the 'Value' column.\n",
        "    Steps:\n",
        "     - Removes '€'\n",
        "     - Strips out weird null or non-ASCII chars\n",
        "     - Handles 'M' (millions) or 'K' (thousands)\n",
        "     - Overwrites the old 'Value' column with a numeric double\n",
        "       (i.e., the column name stays 'Value', but now it's numeric)\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): Source DataFrame with 'Value' as string.\n",
        "    Returns:\n",
        "        DataFrame: A new DataFrame where 'Value' is numeric.\n",
        "    \"\"\"\n",
        "    # Remove \"€\"\n",
        "    temp = df.withColumn(\"Value\", regexp_replace(col(\"Value\"), \"€\", \"\"))\n",
        "\n",
        "    # UDF to parse\n",
        "    def parse_value(str_val):\n",
        "        if not str_val:\n",
        "            return None\n",
        "        # Clean out non-ASCII\n",
        "        cleaned = \"\".join(ch for ch in str_val.strip() if 32 <= ord(ch) <= 126)\n",
        "        if not cleaned:\n",
        "            return None\n",
        "        cleaned = cleaned.upper()\n",
        "        if cleaned.endswith(\"M\"):\n",
        "            try:\n",
        "                return float(cleaned[:-1]) * 1e6\n",
        "            except ValueError:\n",
        "                return None\n",
        "        elif cleaned.endswith(\"K\"):\n",
        "            try:\n",
        "                return float(cleaned[:-1]) * 1e3\n",
        "            except ValueError:\n",
        "                return None\n",
        "        else:\n",
        "            try:\n",
        "                return float(cleaned)\n",
        "            except ValueError:\n",
        "                return None\n",
        "\n",
        "    parse_value_udf = F.udf(parse_value, DoubleType())\n",
        "    temp = temp.withColumn(\"Value\", parse_value_udf(col(\"Value\")))\n",
        "    return temp"
      ],
      "metadata": {
        "id": "hdJs861p1y9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_feature_management():\n",
        "    \"\"\"\n",
        "    Defines the numeric columns we want to cluster on.\n",
        "    We have:\n",
        "        Age\n",
        "        Overall\n",
        "        Potential\n",
        "        Salary (cleaned from 'Wage')\n",
        "        Value  (cleaned from original 'Value')\n",
        "    \"\"\"\n",
        "    return [\n",
        "        (\"Age\", \"Numerical\", \"Use as-is or scale\", \"Example: Age=31 -> scaled=0.45\"),\n",
        "        (\"Overall\", \"Numerical\", \"Use as-is or scale\", \"Example: Overall=94 -> scaled=0.90\"),\n",
        "        (\"Potential\", \"Numerical\", \"Use as-is or scale\", \"Example: Potential=94 -> scaled=0.90\"),\n",
        "        (\"Salary\", \"Numerical\", \"Use as-is or scale\", \"Example: €565K -> 565 -> scaled=0.40\"),\n",
        "        (\"Value\", \"Numerical\", \"Use as-is or scale\", \"Example: €110.5M -> 110500000 -> scaled=1.0\"),\n",
        "    ]"
      ],
      "metadata": {
        "id": "3SP8aMMARiC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_feature_table(feature_management):\n",
        "    \"\"\"\n",
        "    Prints a table that shows how each feature is being managed.\n",
        "    \"\"\"\n",
        "    print(\"Column name | Feature type | Way to manage a feature | Example\")\n",
        "    for col_name, f_type, method, ex in feature_management:\n",
        "        print(f\"{col_name}\\t{f_type}\\t{method}\\t{ex}\")"
      ],
      "metadata": {
        "id": "mn1No_7o11zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_10_players_by_salary(df, salary_col=\"Salary\"):\n",
        "    \"\"\"\n",
        "    Returns the top 10 players by numeric salary in descending order.\n",
        "    \"\"\"\n",
        "    top_10_df = df.orderBy(desc(salary_col)).limit(10)\n",
        "    return top_10_df.collect()"
      ],
      "metadata": {
        "id": "hdGReazL15EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_feature_pipeline(df, feature_cols, k):\n",
        "    \"\"\"\n",
        "    Builds a pipeline:\n",
        "      (1) VectorAssembler -> 'assembledFeatures'\n",
        "      (2) StandardScaler  -> 'scaledFeatures'\n",
        "      (3) KMeans -> 'clusterPrediction'\n",
        "    Returns:\n",
        "      - The fitted pipeline model\n",
        "      - The transformed DataFrame (with cluster predictions)\n",
        "    \"\"\"\n",
        "    # First, cache the filtered DataFrame to ensure consistent partitioning\n",
        "    filtered_df = df.na.drop(subset=feature_cols).cache()\n",
        "\n",
        "    # Repartition to ensure consistent partition sizes\n",
        "    filtered_df = filtered_df.repartition(10)  # You can adjust the number based on your data size\n",
        "\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=feature_cols,\n",
        "        outputCol=\"assembledFeatures\"\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler(\n",
        "        inputCol=\"assembledFeatures\",\n",
        "        outputCol=\"scaledFeatures\",\n",
        "        withMean=True,\n",
        "        withStd=True\n",
        "    )\n",
        "\n",
        "    kmeans = KMeans(\n",
        "        k=k,\n",
        "        featuresCol=\"scaledFeatures\",\n",
        "        predictionCol=\"clusterPrediction\",\n",
        "        seed=42,\n",
        "        maxIter=20  # Added to ensure convergence\n",
        "    )\n",
        "\n",
        "    pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
        "\n",
        "    try:\n",
        "        model = pipeline.fit(filtered_df)\n",
        "        transformed_df = model.transform(filtered_df)\n",
        "        filtered_df.unpersist()  # Clean up cached data\n",
        "        return model, transformed_df\n",
        "    except Exception as e:\n",
        "        filtered_df.unpersist()  # Ensure cleanup even if error occurs\n",
        "        raise e"
      ],
      "metadata": {
        "id": "NQetBn_b16_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_optimal_clusters(df, feature_cols, min_k=2, max_k=8):\n",
        "    \"\"\"\n",
        "    Loops over k in [min_k, max_k], builds pipeline, evaluates\n",
        "    with Silhouette, picks best k.\n",
        "    Returns (best_k, best_score).\n",
        "    \"\"\"\n",
        "    best_k, best_score = None, -1\n",
        "\n",
        "    # Cache the filtered DataFrame to avoid recomputing\n",
        "    filtered_df = df.na.drop(subset=feature_cols).cache()\n",
        "    filtered_df = filtered_df.repartition(10)  # Consistent partitioning\n",
        "\n",
        "    evaluator = ClusteringEvaluator(\n",
        "        predictionCol=\"clusterPrediction\",\n",
        "        featuresCol=\"scaledFeatures\",\n",
        "        metricName=\"silhouette\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        for k in range(min_k, max_k + 1):\n",
        "            print(f\"Testing k={k}...\")  # Added for debugging\n",
        "            model, tdf = build_feature_pipeline(filtered_df, feature_cols, k)\n",
        "            sil = evaluator.evaluate(tdf)\n",
        "            print(f\"Silhouette score for k={k}: {sil}\")  # Added for debugging\n",
        "            if sil > best_score:\n",
        "                best_score = sil\n",
        "                best_k = k\n",
        "\n",
        "        filtered_df.unpersist()\n",
        "        return best_k, best_score\n",
        "    except Exception as e:\n",
        "        filtered_df.unpersist()\n",
        "        raise e"
      ],
      "metadata": {
        "id": "oEzy1KhI2CQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_and_report_top_10(df, feature_cols, k, top_10_players, id_col=\"Name\"):\n",
        "    \"\"\"\n",
        "    Builds the pipeline for given k, then checks how many of the top-10-salary\n",
        "    players fall into each cluster.\n",
        "    \"\"\"\n",
        "    from pyspark.sql.functions import count\n",
        "    model, transformed_df = build_feature_pipeline(df, feature_cols, k)\n",
        "    top_10_names = [row[id_col] for row in top_10_players]\n",
        "    # Only the top-10 players\n",
        "    filter_df = transformed_df.filter(col(id_col).isin(top_10_names))\n",
        "    # Count how many appear in each cluster\n",
        "    grouped = filter_df.groupBy(\"clusterPrediction\").agg(count(\"*\").alias(\"countTop10\"))\n",
        "    grouped = grouped.orderBy(\"clusterPrediction\")\n",
        "    return grouped.withColumnRenamed(\"clusterPrediction\", \"ClustersID\")"
      ],
      "metadata": {
        "id": "EQL93rV0R6jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_pca(df, input_col, output_col, k):\n",
        "    \"\"\"\n",
        "    Applies PCA to 'input_col' -> 'output_col' with k principal components.\n",
        "    Returns new DataFrame with 'output_col'.\n",
        "    \"\"\"\n",
        "    pca = PCA(k=k, inputCol=input_col, outputCol=output_col)\n",
        "    pca_model = pca.fit(df)\n",
        "    return pca_model.transform(df)"
      ],
      "metadata": {
        "id": "H9tEyfkBmco-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_2d_clusters(df, x_col=\"pcaFeatures\", cluster_col=\"pcaClusterPrediction\",\n",
        "                     top_10_ids=None, id_col=\"Name\"):\n",
        "    \"\"\"\n",
        "    Plots clusters in 2D space (first two PCA components).\n",
        "    If top_10_ids is provided, highlight them in red with a label.\n",
        "    \"\"\"\n",
        "    rows = df.select(x_col, cluster_col, id_col).collect()\n",
        "    xs, ys, clusters, names = [], [], [], []\n",
        "    for row in rows:\n",
        "        vec = row[x_col]\n",
        "        if vec and len(vec) >= 2:\n",
        "            xs.append(vec[0])\n",
        "            ys.append(vec[1])\n",
        "        else:\n",
        "            xs.append(0)\n",
        "            ys.append(0)\n",
        "        clusters.append(row[cluster_col])\n",
        "        names.append(row[id_col])\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sc = plt.scatter(xs, ys, c=clusters, cmap=\"viridis\", alpha=0.5)\n",
        "    plt.colorbar(sc, label=\"Cluster ID\")\n",
        "    plt.title(\"Clusters in 2D PCA Space\")\n",
        "    plt.xlabel(\"PCA1\")\n",
        "    plt.ylabel(\"PCA2\")\n",
        "\n",
        "    if top_10_ids:\n",
        "        highlight = [r[id_col] for r in top_10_ids]\n",
        "        for i, nm in enumerate(names):\n",
        "            if nm in highlight:\n",
        "                plt.scatter(xs[i], ys[i], c=\"red\", marker=\"x\", s=100)\n",
        "                plt.text(xs[i]+0.3, ys[i], nm, fontsize=8, color=\"red\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "m3s_eVkVmo0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MAIN EXECUTION\n",
        "All steps from start to finish.\n",
        "\"\"\"\n",
        "file_path = \"dataFIFA.csv\"\n",
        "df_fifa = load_data(file_path)\n",
        "\n",
        "print(\"Preview Data:\")\n",
        "df_fifa.show(5)\n",
        "\n",
        "# 1) Clean 'Wage' -> 'Salary'\n",
        "df_fifa = create_salary_column(df_fifa)\n",
        "\n",
        "# 2) Clean 'Value' -> numeric 'Value' (replaces the old string column with a numeric column of the same name)\n",
        "df_fifa = create_value_column(df_fifa)\n",
        "\n",
        "# Check columns to see that we have \"Salary\" and \"Value\" as doubles now\n",
        "print(\"Columns:\", df_fifa.columns)"
      ],
      "metadata": {
        "id": "tmayFyTVoKwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Identify top 10 players with the biggest Salary.\")\n",
        "top_10_players = get_top_10_players_by_salary(df_fifa, salary_col=\"Salary\")\n",
        "for row in top_10_players:\n",
        "    print(row)\n",
        "\n",
        "print(\"\\nFeature Management Table:\")\n",
        "feature_info = define_feature_management()\n",
        "print_feature_table(feature_info)\n",
        "\n",
        "selected_features = [item[0] for item in feature_info]\n",
        "\n",
        "print(\"\\nFinding the optimal number of clusters via Silhouette.\")"
      ],
      "metadata": {
        "id": "M5KBgRk0oRUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_k, best_score = find_optimal_clusters(df_fifa, selected_features, min_k=2, max_k=8)\n",
        "print(f\"Optimal k = {best_k}, best Silhouette = {best_score:.4f}\")\n",
        "\n",
        "print(\"\\nCreating a result table for that best k.\")\n",
        "res_df = cluster_and_report_top_10(df_fifa, selected_features, best_k, top_10_players, id_col=\"Name\")\n",
        "res_df.show()\n",
        "\n",
        "print(\"\\nApply PCA for 5, 4, 3 principal components & measure Silhouette each time.\")\n",
        "for pc in [5, 4, 3]:\n",
        "    # Filter out any rows missing those features\n",
        "    valid_df = df_fifa.na.drop(subset=selected_features)\n",
        "    _, pipe_output = build_feature_pipeline(valid_df, selected_features, best_k)\n",
        "    # Now pipe_output has 'scaledFeatures'\n",
        "    pca_output = apply_pca(pipe_output, \"scaledFeatures\", \"pcaFeatures\", pc)\n",
        "    # Re-run KMeans on pcaFeatures\n",
        "    kmeans = KMeans(k=best_k, featuresCol=\"pcaFeatures\", predictionCol=\"pcaClusterPrediction\", seed=42)\n",
        "    kmodel = kmeans.fit(pca_output)\n",
        "    final_clust = kmodel.transform(pca_output)\n",
        "\n",
        "    evaluator = ClusteringEvaluator(\n",
        "        predictionCol=\"pcaClusterPrediction\",\n",
        "        featuresCol=\"pcaFeatures\",\n",
        "        metricName=\"silhouette\"\n",
        "    )\n",
        "    sil_score = evaluator.evaluate(final_clust)\n",
        "    print(f\"With {pc} PCA components, Silhouette = {sil_score:.4f}\")\n",
        "\n",
        "print(\"\\nFinally, reduce to 2 PCA components and plot the clusters.\")\n",
        "valid_df = df_fifa.na.drop(subset=selected_features)\n",
        "_, pipe_output = build_feature_pipeline(valid_df, selected_features, best_k)\n",
        "pca_2d = apply_pca(pipe_output, \"scaledFeatures\", \"pcaFeatures\", 2)\n",
        "\n",
        "km2d = KMeans(k=best_k, featuresCol=\"pcaFeatures\", predictionCol=\"pcaClusterPrediction\", seed=42)\n",
        "km2d_model = km2d.fit(pca_2d)\n",
        "pca_2d_final = km2d_model.transform(pca_2d)\n",
        "\n",
        "plot_2d_clusters(\n",
        "    pca_2d_final,\n",
        "    x_col=\"pcaFeatures\",\n",
        "    cluster_col=\"pcaClusterPrediction\",\n",
        "    top_10_ids=top_10_players,\n",
        "    id_col=\"Name\"\n",
        ")\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "4CdUJoC_n_u1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}